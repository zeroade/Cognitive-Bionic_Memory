#!/usr/bin/env python3
"""
CBMA â€” Cognitive-Bionic Memory Architecture
Interactive CLI Demo

Commands:
  /help                  Show commands
  /alias <term> = <meaning>   Set temporary binding
  /aliases               Show active bindings
  /buffer                Show attention buffer state
  /buffer detail         Show buffer + compression history
  /search <query>        Search dual store (Layer 1)
  /kg <concept>          Query knowledge graph directly (Layer 0)
  /consolidate           Run consolidation cycle (Through-Axis)
  /scores                Show saliency scores for all episodes
  /episodes              List all episodic memories
  /semantics             List all semantic entries
  /status                Show full system status
  /quit                  Exit

Any other input is treated as a conversational query that
flows through all layers: L0 â†’ L1 â†’ L2 â†’ L3.
"""

import os
import sys
import json

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from layer0_representation import KnowledgeGraph, MockLLM, ArbitrationLayer
from layer1_storage import EpisodicStore, SemanticStore, TemporaryBinding, DualStore
from layer2_processing import AttentionBuffer, PhonologicalLoop
from layer3_output import CognitiveLoadMonitor, OutputRegulator
from consolidation_engine import SaliencyScorer, ConsolidationEngine

# â”€â”€â”€ ANSI colors â”€â”€â”€
class C:
    RESET   = "\033[0m"
    BOLD    = "\033[1m"
    DIM     = "\033[2m"
    RED     = "\033[91m"
    GREEN   = "\033[92m"
    YELLOW  = "\033[93m"
    BLUE    = "\033[94m"
    MAGENTA = "\033[95m"
    CYAN    = "\033[96m"
    WHITE   = "\033[97m"


def colored(text, color):
    return f"{color}{text}{C.RESET}"


def header(text):
    width = 60
    print(colored(f"\n{'â”€' * width}", C.DIM))
    print(colored(f"  {text}", C.BOLD + C.CYAN))
    print(colored(f"{'â”€' * width}", C.DIM))


def subheader(text):
    print(colored(f"\n  â—† {text}", C.BOLD + C.YELLOW))


def info(text):
    print(colored(f"  {text}", C.WHITE))


def dim(text):
    print(colored(f"  {text}", C.DIM))


def success(text):
    print(colored(f"  âœ“ {text}", C.GREEN))


def warn(text):
    print(colored(f"  âš  {text}", C.YELLOW))


def error(text):
    print(colored(f"  âœ— {text}", C.RED))


class CBMA:
    """Main system that orchestrates all layers."""

    def __init__(self, data_dir: str):
        # Layer 0: Representation
        self.kg = KnowledgeGraph(os.path.join(data_dir, "knowledge_graph.json"))
        self.llm = MockLLM()
        self.arbitration = ArbitrationLayer(self.kg, self.llm)

        # Layer 1: Storage
        self.episodic = EpisodicStore(os.path.join(data_dir, "episodic_store.json"))
        self.semantic = SemanticStore(os.path.join(data_dir, "semantic_store.json"))
        self.binding = TemporaryBinding()
        self.dual_store = DualStore(self.episodic, self.semantic, self.binding)

        # Layer 2: Processing
        self.buffer = AttentionBuffer(capacity=5)
        self.phono_loop = PhonologicalLoop(cycle_threshold=3)

        # Layer 3: Output
        self.load_monitor = CognitiveLoadMonitor()
        self.regulator = OutputRegulator(self.load_monitor)

        # Through-Axis
        self.scorer = SaliencyScorer()
        self.consolidation = ConsolidationEngine(self.scorer)

        # Conversation state
        self.turn_count = 0

    def process_query(self, query: str):
        """Full pipeline: query flows through all layers."""
        self.turn_count += 1
        header(f"Query #{self.turn_count}: {query[:50]}{'...' if len(query) > 50 else ''}")

        # â”€â”€ Layer 0: Arbitration â”€â”€
        subheader("Layer 0 â€” è¡¨å¾µå±¤ï¼šConfidence-Gated ä»²è£")
        arb_result = self.arbitration.arbitrate(query)
        info(f"æ±ºç­–ï¼š{arb_result['decision']}")
        info(f"èªªæ˜ï¼š{arb_result['explanation']}")
        if arb_result['kg_hits'] > 0:
            dim(f"KG å‘½ä¸­ {arb_result['kg_hits']} æ¢ï¼Œæœ€é«˜ä¿¡å¿ƒ {arb_result['kg_max_confidence']:.2f}")
            for t in arb_result['trace']['kg_results'][:3]:
                dim(f"  {t['subject']} â€”[{t['relation']}]â†’ {t['object']} ({t['confidence']:.2f})")

        # â”€â”€ Layer 1: Dual Store Search â”€â”€
        subheader("Layer 1 â€” å­˜å„²å±¤ï¼šé›™å­˜å„²åº«æª¢ç´¢")
        store_result = self.dual_store.search(query)
        info(f"ç­–ç•¥ï¼š{store_result['strategy']}")
        info(f"èªªæ˜ï¼š{store_result['explanation']}")
        if store_result['active_aliases']:
            dim(f"æ´»èºåˆ¥åï¼š{store_result['active_aliases']}")

        if store_result['episodic_results']:
            dim(f"æƒ…ç¯€è¨˜æ†¶å‘½ä¸­ {len(store_result['episodic_results'])} æ¢ï¼š")
            for ep in store_result['episodic_results'][:2]:
                dim(f"  [{ep['id']}] {ep['source'][:40]} (ç›¸é—œåº¦: {ep['_relevance']:.2f})")
        if store_result['semantic_results']:
            dim(f"èªç¾©è¨˜æ†¶å‘½ä¸­ {len(store_result['semantic_results'])} æ¢ï¼š")
            for sem in store_result['semantic_results'][:2]:
                dim(f"  [{sem['id']}] {sem['concept']} (ç›¸é—œåº¦: {sem['_relevance']:.2f})")

        # â”€â”€ Layer 2: Attention Buffer â”€â”€
        subheader("Layer 2 â€” è™•ç†å±¤ï¼šæ³¨æ„åŠ›ç·©è¡å€")
        # Extract key concepts: prefer KG-matched concepts, fall back to short terms
        concepts = arb_result.get("concepts_searched", [])
        # Filter out overly long concepts (likely full phrases, not concepts)
        concepts = [c for c in concepts if len(c) <= 20]
        if not concepts:
            # Fallback: extract short meaningful terms
            import re
            tokens = re.split(r'[\sï¼Œã€‚ï¼Ÿï¼ã€/]+', query)
            concepts = [t for t in tokens if 2 <= len(t) <= 15][:3]
        for concept in concepts[:3]:
            buf_result = self.buffer.add(concept, query, "conversation")
            info(buf_result["message"])

            # Phonological loop for new concepts
            loop_result = self.phono_loop.encounter(concept)
            if loop_result["status"] in ("new", "cycling"):
                dim(loop_result["message"])
            elif loop_result["status"] == "consolidated":
                success(loop_result["message"])

        # â”€â”€ Layer 3: Output Regulation â”€â”€
        subheader("Layer 3 â€” è¼¸å‡ºå±¤ï¼šèªçŸ¥è² è·è©•ä¼°")
        # Build a mock response combining all results
        response_parts = []
        if arb_result['response']:
            response_parts.append(arb_result['response'])
        for ep in store_result['episodic_results'][:2]:
            response_parts.append(ep['content'][:100])
        for sem in store_result['semantic_results'][:1]:
            response_parts.append(sem['content'][:100])
        mock_response = "\n".join(response_parts) if response_parts else "ï¼ˆç„¡ç›¸é—œè³‡è¨Šï¼‰"

        known = [c.concept for c in self.buffer.buffer]
        regulation = self.regulator.regulate(mock_response, known)
        info(regulation["assessment"]["message"])
        if regulation["was_regulated"]:
            warn("å›æ‡‰å·²é€²è¡ŒèªçŸ¥è² è·èª¿ç¯€")

        # â”€â”€ Final Output â”€â”€
        subheader("ç¶œåˆå›æ‡‰")
        print()
        for line in regulation["regulated_response"].split("\n"):
            if line.strip():
                info(line)
        print()

    def cmd_alias(self, args: str):
        """Handle /alias command."""
        if "=" not in args:
            error("ç”¨æ³•ï¼š/alias <term> = <meaning>")
            error("ä¾‹å¦‚ï¼š/alias ç“¶é ¸ = å·¥ä½œè¨˜æ†¶é™åˆ¶å¸¶ä¾†çš„æ­£é¢ç´„æŸ")
            return
        parts = args.split("=", 1)
        term = parts[0].strip()
        meaning = parts[1].strip()
        result = self.binding.add_alias(term, meaning)
        success(result)

    def cmd_aliases(self):
        """Show active aliases."""
        aliases = self.binding.get_aliases()
        if not aliases:
            info("ç›®å‰æ²’æœ‰æ´»èºçš„è‡¨æ™‚ç¶å®š")
        else:
            header("æ´»èºçš„è‡¨æ™‚ç¶å®š")
            for term, meaning in aliases.items():
                info(f"ã€Œ{term}ã€â†’ã€Œ{meaning}ã€")

    def cmd_buffer(self, detail: bool = False):
        """Show buffer state."""
        header("æ³¨æ„åŠ›ç·©è¡å€ç‹€æ…‹")
        state = self.buffer.get_state()
        if not state:
            info("ç·©è¡å€ç‚ºç©º")
            return
        for i, chunk in enumerate(state):
            marker = "ğŸ“¦" if chunk["compressed"] else "ğŸ’¡"
            info(f"{marker} [{i+1}] {chunk['concept']}")
            dim(f"     {chunk['content']}")
            if chunk["compressed"] and chunk["contains"]:
                dim(f"     åŒ…å«ï¼š{', '.join(chunk['contains'])}")
            dim(f"     ä¾†æºï¼š{chunk['source']} | å­˜å–æ¬¡æ•¸ï¼š{chunk['accesses']}")
        info(f"\n  å®¹é‡ï¼š{len(state)}/{self.buffer.capacity}")

        cycling = self.phono_loop.get_cycling_concepts()
        if cycling:
            subheader("èªéŸ³ç’°è·¯ â€” å¾ªç’°æ¿€æ´»ä¸­")
            for concept, count in cycling.items():
                dim(f"  ğŸ”„ {concept} [{count}/{self.phono_loop.cycle_threshold}]")

        if detail:
            history = self.buffer.get_compression_history()
            if history:
                subheader("å£“ç¸®æ­·å²")
                for event in history:
                    dim(f"  {event['timestamp'][:19]}")
                    dim(f"    æ·˜æ±°ï¼š{event['evicted']}")
                    dim(f"    å£“ç¸®ç‚ºï¼š{event['compressed_into']}")
                    dim(f"    æ–°å¢ï¼š{event['new_concept']}")

    def cmd_kg(self, concept: str):
        """Query KG directly."""
        header(f"çŸ¥è­˜åœ–è­œæŸ¥è©¢ï¼š{concept}")
        results = self.kg.query(concept)
        if not results:
            info("ç„¡åŒ¹é…è¨˜éŒ„")
            return
        for r in results:
            conf_color = C.GREEN if r['confidence'] >= 0.85 else (C.YELLOW if r['confidence'] >= 0.5 else C.RED)
            conf_val = r['confidence']
            print(f"  {r['subject']} â€”[{r['relation']}]â†’ {r['object']}  {colored(f'{conf_val:.2f}', conf_color)}")

    def cmd_search(self, query: str):
        """Direct dual-store search."""
        header(f"é›™å­˜å„²åº«æœå°‹ï¼š{query}")
        result = self.dual_store.search(query)
        info(f"ç­–ç•¥ï¼š{result['strategy']} â€” {result['explanation']}")
        if result['episodic_results']:
            subheader("æƒ…ç¯€è¨˜æ†¶")
            for ep in result['episodic_results'][:5]:
                info(f"[{ep['id']}] {ep['source'][:50]}")
                dim(f"  {ep['content'][:80]}...")
        if result['semantic_results']:
            subheader("èªç¾©è¨˜æ†¶")
            for sem in result['semantic_results'][:3]:
                info(f"[{sem['id']}] {sem['concept']}")
                dim(f"  {sem['content'][:80]}...")

    def cmd_consolidate(self):
        """Run consolidation cycle."""
        header("éå›º-éºå¿˜-é‡çµ„å¼•æ“ å•Ÿå‹•")

        # First show scores
        scores = self.consolidation.get_score_report(self.episodic, self.semantic)
        subheader("é¡¯è‘—æ€§è©•åˆ†")
        for s in scores:
            action_color = C.GREEN if s['action'] == 'consolidate' else (C.RED if s['action'] == 'prune' else C.YELLOW)
            action_marker = {"consolidate": "â¬† éå›º", "prune": "â¬‡ éºå¿˜", "retain": "â— ä¿ç•™"}[s['action']]
            print(f"  {colored(action_marker, action_color)}  {s['episode_id']}  "
                  f"score={s['total_score']:.3f}  {s['source']}")
            dims = s['dimensions']
            dim(f"    freq={dims['frequency']:.2f} recency={dims['recency']:.2f} "
                f"user={dims['user_signal']:.2f} novelty={dims['novelty']:.2f} "
                f"connect={dims['connection_density']:.2f}")

        # Execute
        subheader("åŸ·è¡Œéå›º")
        result = self.consolidation.run(self.episodic, self.semantic)
        success(f"å·²è©•ä¼° {result['total_scored']} æ¢æƒ…ç¯€è¨˜æ†¶")
        if result['consolidated'] > 0:
            success(f"éå›º {result['consolidated']} æ¢è‡³èªç¾©è¨˜æ†¶ï¼š")
            for c in result['details']['consolidated']:
                dim(f"  {c['source_episode']} â†’ {c['new_semantic_entry']}: {c['concept']}")
        if result['pruned'] > 0:
            warn(f"éºå¿˜ {result['pruned']} æ¢ï¼š{result['details']['pruned']}")
        if result['retained'] > 0:
            info(f"ä¿ç•™ {result['retained']} æ¢åœ¨æƒ…ç¯€å­˜å„²ä¸­")

    def cmd_scores(self):
        """Show saliency scores without executing."""
        header("é¡¯è‘—æ€§è©•åˆ†ï¼ˆé è¦½ï¼Œä¸åŸ·è¡Œï¼‰")
        scores = self.consolidation.get_score_report(self.episodic, self.semantic)
        for s in scores:
            action_color = C.GREEN if s['action'] == 'consolidate' else (C.RED if s['action'] == 'prune' else C.YELLOW)
            action_marker = {"consolidate": "â¬†", "prune": "â¬‡", "retain": "â—"}[s['action']]
            print(f"  {colored(action_marker, action_color)} {s['total_score']:.3f}  "
                  f"{s['episode_id']}  {s['source']}")

    def cmd_episodes(self):
        """List episodic store."""
        header(f"æƒ…ç¯€å­˜å„²åº«ï¼ˆ{len(self.episodic.episodes)} æ¢ï¼‰")
        for ep in self.episodic.episodes:
            info(f"[{ep['id']}] {ep['timestamp'][:10]}  {ep['source'][:50]}")
            dim(f"  é‡è¦æ€§: {ep.get('user_importance', '?')}/5  "
                f"æª¢ç´¢æ¬¡æ•¸: {ep.get('retrieval_count', 0)}  "
                f"æ¨™ç±¤: {', '.join(ep.get('tags', []))}")

    def cmd_semantics(self):
        """List semantic store."""
        header(f"èªç¾©å­˜å„²åº«ï¼ˆ{len(self.semantic.entries)} æ¢ï¼‰")
        for entry in self.semantic.entries:
            info(f"[{entry['id']}] {entry['concept']}")
            dim(f"  {entry['content'][:80]}...")
            dim(f"  ä¿¡å¿ƒåº¦: {entry.get('confidence', '?')}  ä¾†æº: {entry.get('source_episodes', [])}")

    def cmd_status(self):
        """Full system status."""
        header("CBMA ç³»çµ±ç‹€æ…‹")
        subheader("Layer 0 â€” çŸ¥è­˜åœ–è­œ")
        info(f"ä¸‰å…ƒçµ„æ•¸é‡ï¼š{len(self.kg.triples)}")

        subheader("Layer 1 â€” é›™å­˜å„²åº«")
        info(f"æƒ…ç¯€è¨˜æ†¶ï¼š{len(self.episodic.episodes)} æ¢")
        info(f"èªç¾©è¨˜æ†¶ï¼š{len(self.semantic.entries)} æ¢")
        aliases = self.binding.get_aliases()
        info(f"è‡¨æ™‚ç¶å®šï¼š{len(aliases)} æ¢")
        if aliases:
            for t, m in aliases.items():
                dim(f"  ã€Œ{t}ã€â†’ã€Œ{m}ã€")

        subheader("Layer 2 â€” æ³¨æ„åŠ›ç·©è¡å€")
        state = self.buffer.get_state()
        info(f"ä½¿ç”¨é‡ï¼š{len(state)}/{self.buffer.capacity}")
        for chunk in state:
            marker = "ğŸ“¦" if chunk["compressed"] else "ğŸ’¡"
            dim(f"  {marker} {chunk['concept']}")
        cycling = self.phono_loop.get_cycling_concepts()
        if cycling:
            info(f"å¾ªç’°æ¿€æ´»ä¸­ï¼š{list(cycling.keys())}")

        subheader("Layer 3 â€” è¼¸å‡ºèª¿ç¯€")
        info(f"å¯†åº¦é–¾å€¼ï¼š{self.load_monitor.density_threshold}")
        info(f"æ–°æ¦‚å¿µä¸Šé™ï¼š{self.load_monitor.max_new_concepts}")

        subheader("è²«ç©¿è»¸ â€” éå›ºå¼•æ“")
        info(f"æ­·å²éå›ºæ¬¡æ•¸ï¼š{len(self.consolidation.consolidation_history)}")
        info(f"å°è©±è¼ªæ¬¡ï¼š{self.turn_count}")

    def cmd_help(self):
        header("CBMA æŒ‡ä»¤åˆ—è¡¨")
        commands = [
            ("/help", "é¡¯ç¤ºæ­¤èªªæ˜"),
            ("/alias <term> = <meaning>", "è¨­å®šè‡¨æ™‚ç¶å®šï¼ˆèªç¾©æ¼‚ç§»è¿½è¹¤ï¼‰"),
            ("/aliases", "é¡¯ç¤ºæ‰€æœ‰æ´»èºç¶å®š"),
            ("/buffer", "é¡¯ç¤ºæ³¨æ„åŠ›ç·©è¡å€ç‹€æ…‹"),
            ("/buffer detail", "é¡¯ç¤ºç·©è¡å€ + å£“ç¸®æ­·å²"),
            ("/search <query>", "ç›´æ¥æœå°‹é›™å­˜å„²åº«"),
            ("/kg <concept>", "ç›´æ¥æŸ¥è©¢çŸ¥è­˜åœ–è­œ"),
            ("/consolidate", "åŸ·è¡Œéå›º-éºå¿˜-é‡çµ„å¾ªç’°"),
            ("/scores", "é è¦½é¡¯è‘—æ€§è©•åˆ†ï¼ˆä¸åŸ·è¡Œï¼‰"),
            ("/episodes", "åˆ—å‡ºæ‰€æœ‰æƒ…ç¯€è¨˜æ†¶"),
            ("/semantics", "åˆ—å‡ºæ‰€æœ‰èªç¾©è¨˜æ†¶"),
            ("/status", "é¡¯ç¤ºå®Œæ•´ç³»çµ±ç‹€æ…‹"),
            ("/quit", "çµæŸ"),
            ("ï¼ˆå…¶ä»–ä»»ä½•è¼¸å…¥ï¼‰", "ä½œç‚ºæŸ¥è©¢ï¼Œæµç¶“å…¨éƒ¨å››å±¤è™•ç†"),
        ]
        for cmd, desc in commands:
            print(f"  {colored(cmd, C.CYAN):40s} {desc}")


def main():
    data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
    system = CBMA(data_dir)

    print(colored("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   CBMA â€” Cognitive-Bionic Memory Architecture           â•‘
â•‘   èªçŸ¥ä»¿ç”Ÿè¨˜æ†¶æ¶æ§‹ äº’å‹•æ¼”ç¤º                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  å››å±¤æ¶æ§‹ + ä¸€è»¸                                         â•‘
â•‘  L0 è¡¨å¾µ â†’ L1 å­˜å„² â†’ L2 è™•ç† â†’ L3 è¼¸å‡º                  â•‘
â•‘  è²«ç©¿è»¸ï¼šéå›º-éºå¿˜-é‡çµ„                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  è¼¸å…¥ /help æŸ¥çœ‹æŒ‡ä»¤   è¼¸å…¥ä»»ä½•å•é¡Œé–‹å§‹äº’å‹•                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """, C.CYAN))

    info(f"å·²è¼‰å…¥ {len(system.kg.triples)} æ¢çŸ¥è­˜åœ–è­œä¸‰å…ƒçµ„")
    info(f"å·²è¼‰å…¥ {len(system.episodic.episodes)} æ¢æƒ…ç¯€è¨˜æ†¶")
    info(f"å·²è¼‰å…¥ {len(system.semantic.entries)} æ¢èªç¾©è¨˜æ†¶")
    print()

    while True:
        try:
            user_input = input(colored("CBMA > ", C.BOLD + C.GREEN)).strip()
        except (EOFError, KeyboardInterrupt):
            print(colored("\n\n  å†è¦‹ï¼", C.CYAN))
            break

        if not user_input:
            continue

        # Command routing
        lower = user_input.lower()

        if lower == "/quit" or lower == "/exit":
            print(colored("\n  å†è¦‹ï¼", C.CYAN))
            break
        elif lower == "/help":
            system.cmd_help()
        elif lower.startswith("/alias "):
            system.cmd_alias(user_input[7:])
        elif lower == "/aliases":
            system.cmd_aliases()
        elif lower == "/buffer detail":
            system.cmd_buffer(detail=True)
        elif lower == "/buffer":
            system.cmd_buffer()
        elif lower.startswith("/search "):
            system.cmd_search(user_input[8:])
        elif lower.startswith("/kg "):
            system.cmd_kg(user_input[4:])
        elif lower == "/consolidate":
            system.cmd_consolidate()
        elif lower == "/scores":
            system.cmd_scores()
        elif lower == "/episodes":
            system.cmd_episodes()
        elif lower == "/semantics":
            system.cmd_semantics()
        elif lower == "/status":
            system.cmd_status()
        else:
            # Treat as conversational query â€” full pipeline
            system.process_query(user_input)


if __name__ == "__main__":
    main()
